{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1vK77D+M4U13o38MVYRKU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h777arsh/PyTorch-Tutorial/blob/main/MLP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6lYWFyGkx-i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip6KvOdklJaf",
        "outputId": "470de40f-0769-4cb8-8945-a7b21907eccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-02 11:31:10--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-06-02 11:31:10 (11.3 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmsV2CpnlQOT",
        "outputId": "3dd18e84-93b4-4278-be5a-2f1026898688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8B9vorplRtD",
        "outputId": "68fa01c0-eccb-4735-dbf9-b2df9c153c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhUfzVMblTQd",
        "outputId": "e6665c1d-5b91-401f-9b90-481cf93d2f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N56NGnAClT-7",
        "outputId": "959879b0-6416-466b-b3d8-5d85c1ca5de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP revisited\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
        "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
        "\n",
        "# BatchNorm parameters\n",
        "bngain = torch.ones((1, n_hidden))\n",
        "bnbias = torch.zeros((1, n_hidden))\n",
        "bnmean_running = torch.zeros((1, n_hidden))\n",
        "bnstd_running = torch.ones((1, n_hidden))\n",
        "\n",
        "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUvAB3H5lWMo",
        "outputId": "95c2c5f3-369f-4023-d395-e1cd0df7e7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmeani = hpreact.mean(0, keepdim=True)\n",
        "  bnstdi = hpreact.std(0, keepdim=True)\n",
        "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
        "  with torch.no_grad():\n",
        "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
        "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYRFCTEblcTU",
        "outputId": "11ac484a-61ac-43fd-c92a-d61572999fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3239\n",
            "  10000/ 200000: 2.0322\n",
            "  20000/ 200000: 2.5675\n",
            "  30000/ 200000: 2.0125\n",
            "  40000/ 200000: 2.2446\n",
            "  50000/ 200000: 1.8897\n",
            "  60000/ 200000: 2.0785\n",
            "  70000/ 200000: 2.3681\n",
            "  80000/ 200000: 2.2918\n",
            "  90000/ 200000: 2.0238\n",
            " 100000/ 200000: 2.3673\n",
            " 110000/ 200000: 2.3132\n",
            " 120000/ 200000: 1.6414\n",
            " 130000/ 200000: 1.9311\n",
            " 140000/ 200000: 2.2231\n",
            " 150000/ 200000: 2.0027\n",
            " 160000/ 200000: 2.0997\n",
            " 170000/ 200000: 2.4949\n",
            " 180000/ 200000: 2.0199\n",
            " 190000/ 200000: 2.1707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "uRwioa01v9hD",
        "outputId": "2bd3e15e-f63e-4d3d-ce6d-a81c7ff935fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7d7d8566ec50>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 # + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnstd = hpreact.std(0, keepdim=True)\n"
      ],
      "metadata": {
        "id": "SkNQul5TwYQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 # + b1\n",
        "  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
        "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "id": "X0-a5097whsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss log\n",
        "original:\n",
        "train 2.1245384216308594 val 2.168196439743042\n",
        "\n",
        "fix softmax confidently wrong:\n",
        "train 2.07 val 2.13\n",
        "\n",
        "fix tanh layer too saturated at init:\n",
        "train 2.0355966091156006 val 2.1026785373687744\n",
        "\n",
        "use semi-principled \"kaiming init\" instead of hacky init:\n",
        "train 2.0376641750335693 val 2.106989622116089\n",
        "\n",
        "add batch norm layer\n",
        "train 2.0668270587921143 val 2.104844808578491"
      ],
      "metadata": {
        "id": "7aVl6xufb9S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY + PYTORCHIFYING -----------"
      ],
      "metadata": {
        "id": "Pg6JdL1Jb-Lh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Let's train a deeper network\n",
        "# The classes we create here are the same API as nn.Module in PyTorch\n",
        "\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # batch mean\n",
        "      xvar = x.var(0, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
        "]\n",
        "# layers = [\n",
        "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
        "#   Linear(           n_hidden, vocab_size),\n",
        "# ]\n",
        "\n",
        "with torch.no_grad():\n",
        "  # last layer: make less confident\n",
        "  layers[-1].gamma *= 0.1\n",
        "  #layers[-1].weight *= 0.1\n",
        "  # all other layers: apply gain\n",
        "  for layer in layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 1.0 #5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "mfApNIVZcBC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for layer in layers:\n",
        "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
        "\n",
        "  if i >= 1000:\n",
        "    break # AFTER_DEBUG: would take out obviously to run full optimization"
      ],
      "metadata": {
        "id": "7oYUkcOBcHSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
        "  if isinstance(layer, Tanh):\n",
        "    t = layer.out\n",
        "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('activation distribution')"
      ],
      "metadata": {
        "id": "dbcMahfAcH77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
        "  if isinstance(layer, Tanh):\n",
        "    t = layer.out.grad\n",
        "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('gradient distribution')"
      ],
      "metadata": {
        "id": "gg5NwNVLcKf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
        "legends = []\n",
        "for i,p in enumerate(parameters):\n",
        "  t = p.grad\n",
        "  if p.ndim == 2:\n",
        "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'{i} {tuple(p.shape)}')\n",
        "plt.legend(legends)\n",
        "plt.title('weights gradient distribution');"
      ],
      "metadata": {
        "id": "UJcuGEXocKYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 4))\n",
        "legends = []\n",
        "for i,p in enumerate(parameters):\n",
        "  if p.ndim == 2:\n",
        "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
        "    legends.append('param %d' % i)\n",
        "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
        "plt.legend(legends);\n"
      ],
      "metadata": {
        "id": "JzKT3Lm2cMIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "# put layers into eval mode\n",
        "for layer in layers:\n",
        "  layer.training = False\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "id": "hERncySXcMGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass the neural net\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
        "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "      for layer in layers:\n",
        "        x = layer(x)\n",
        "      logits = x\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      # sample from the distribution\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      # shift the context window and track the samples\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      # if we sample the special '.' token, break\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
      ],
      "metadata": {
        "id": "8Uk4j6F6cPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BatchNorm forward pass as a widget\n",
        "\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "def normshow(x0):\n",
        "\n",
        "  g = torch.Generator().manual_seed(2147483647+1)\n",
        "  x = torch.randn(5, generator=g) * 5\n",
        "  x[0] = x0 # override the 0th example with the slider\n",
        "  mu = x.mean()\n",
        "  sig = x.std()\n",
        "  y = (x - mu)/sig\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  # plot 0\n",
        "  plt.plot([-6,6], [0,0], 'k')\n",
        "  # plot the mean and std\n",
        "  xx = np.linspace(-6, 6, 100)\n",
        "  plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')\n",
        "  xx = np.linspace(-6, 6, 100)\n",
        "  plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')\n",
        "  # plot little lines connecting input and output\n",
        "  for i in range(len(x)):\n",
        "    plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)\n",
        "  # plot the input and output values\n",
        "  plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)\n",
        "  plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)\n",
        "  plt.xlim(-6, 6)\n",
        "  # title\n",
        "  plt.title('input mu %.2f std %.2f' % (mu, sig))\n",
        "\n",
        "interact(normshow, x0=(-30,30,0.5));\n"
      ],
      "metadata": {
        "id": "imjEsiBBcR-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear: activation statistics of forward and backward pass\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "a = torch.randn((1000,1), requires_grad=True, generator=g)          # a.grad = b.T @ c.grad\n",
        "b = torch.randn((1000,1000), requires_grad=True, generator=g)       # b.grad = c.grad @ a.T\n",
        "c = b @ a\n",
        "loss = torch.randn(1000, generator=g) @ c\n",
        "a.retain_grad()\n",
        "b.retain_grad()\n",
        "c.retain_grad()\n",
        "loss.backward()\n",
        "print('a std:', a.std().item())\n",
        "print('b std:', b.std().item())\n",
        "print('c std:', c.std().item())\n",
        "print('-----')\n",
        "print('c grad std:', c.grad.std().item())\n",
        "print('a grad std:', a.grad.std().item())\n",
        "print('b grad std:', b.grad.std().item())"
      ],
      "metadata": {
        "id": "Ol3Qee6DcUAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear + BatchNorm: activation statistics of forward and backward pass\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "n = 1000\n",
        "# linear layer ---\n",
        "inp = torch.randn(n, requires_grad=True, generator=g)\n",
        "w = torch.randn((n, n), requires_grad=True, generator=g) # / n**0.5\n",
        "x = w @ inp\n",
        "# bn layer ---\n",
        "xmean = x.mean()\n",
        "xvar = x.var()\n",
        "out = (x - xmean) / torch.sqrt(xvar + 1e-5)\n",
        "# ----\n",
        "loss = out @ torch.randn(n, generator=g)\n",
        "inp.retain_grad()\n",
        "x.retain_grad()\n",
        "w.retain_grad()\n",
        "out.retain_grad()\n",
        "loss.backward()\n",
        "\n",
        "print('inp std: ', inp.std().item())\n",
        "print('w std: ', w.std().item())\n",
        "print('x std: ', x.std().item())\n",
        "print('out std: ', out.std().item())\n",
        "print('------')\n",
        "print('out grad std: ', out.grad.std().item())\n",
        "print('x grad std: ', x.grad.std().item())\n",
        "print('w grad std: ', w.grad.std().item())\n",
        "print('inp grad std: ', inp.grad.std().item())"
      ],
      "metadata": {
        "id": "iR1uRv1dcVLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}